## ES的基本概念
1. 什么是ElasticSearch
    - **基于Lucene的搜索型数据库**，它提供了近乎实时的搜索。
    - **倒排索引+字段树**。实现搜索技术+加快关键字的索引。
    - **DocValue+FieldData缓存**。加快排序和聚合的速度
    - **集群管理**。支持动态地添加/删除机器，支持PB级别的数据处理


2. ES数据管理方式
    - **索引 => 类型 => 文档 => 字段**
    - **类型(type)只是ES的概念，方便数据管理**。ES为每个类型维护了一个mapping，mapping定义了该类型下文档的字段类型。字段类型一旦确定下来就无法插入其它类型的字段。Lucene底层是没有类型概念的，只有索引和文档
    - **ES建议同一个索引下只建议建一个类型**，以减少lucene存储的空值（因为lucene索引会存储ES索引下所有类型的字段）。经常会把ES的存储结构与mysql做对比。索引 -> 数据库，类型 - > 表，字段 -> 列，文档 -> 一行值。


3. ES有什么缺点？
    - **字段类型无法改变**。类型mapping中对某个字段类型确定后，就无法写入其它类型的值。如果要改变只能重新索引所有数据
    - **索引分片数量无法改变**。需要事先规划好索引分片数量，后期如果要增加分片数量，只能重新索引所有数据
    - **不支持事务**。
    - **内存要求高**。它本质上是高效利用内存缓存来提高读写速度，因此对内存要求很高。


4. ES中的倒排索引是什么？
    - 传统的检索方式是通过文章，逐个遍历找到对应关键词的位置。
　　倒排索引，是通过分词策略，形成了词和文章的映射关系表，也称倒排表，这种词典+映射表即为倒排索引。
　　其中词典中存储词元，倒排表中存储该词元在哪些文中出现的位置。
　　有了倒排索引，就能实现O(1)时间复杂度的效率检索文章了，极大的提高了检索效率。
    - 加分项：倒排索引的底层实现是基于：FST（Finite State Transducer）数据结构。Lucene从4+版本后开始大量使用的数据结构是FST。FST有两个优点：1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；2）查询速度快。O(len(str))的查询时间复杂度


5. lucence内部结构是什么？
![](https://raw.githubusercontent.com/roperluo32/images/master/image20191212075445.png)
Lucene是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。


6. Lucene加快查询的机制有哪些？
    - 倒排索引，可以快速检索关键字所在的文档
    - 使用字典树加速关键字的查找
    - 使用FST实现字典树，减少了字典树的存储空间


7. Elasticsearch的缺点有哪些？你觉得可以在哪些地方进行改进？
    - 不支持事务
    - 对内存需求比较大
    - 安全性比较差
    - 字段类型无法修改
    - 写入性能低（会建全文索引，term字典和更新内存中的term索引）


8. query和filter的区别
    - query会计算相关性得分，计算结果不能缓存，效率比较低
    - filter只关心匹不匹配，不计算得分，计算结构能缓存，效率比较高


9. DocValues的作用
    - DocValues是为了加快根据字段去做group，排序而新增的一个正排索引（文档id到字段），类似列存储。使用压缩技术存储每一个字段的内容

10. 一个索引下多个type可能会导致什么样的问题
    - type只是elastic层面的概念，lucene下只有索引的概念。因此同一个索引下面建太多不同的type会导致lucene存储太多的空字段，导致存储检索的效率下降


11. ES内部索引的实现
    - 使用字典树实现的内部索引，通过字典树可以快速定位到某个key
    - 字典树使用FST实现，极大地压缩了字段的大小
    - 另外建立了DocValue的正排索引，可以快速的基于文档的某个字段做group和排序

12. 是否了解字典树
    - 常用字典数据结构如下所示
    ![](https://raw.githubusercontent.com/roperluo32/images/master/image20191213072608.png)

## ES集群的管理
1. 节点有哪些类型
     - **master**。节点是备选master，可以参与master竞选
     - **data**。节点只能是数据节点，用来存储数据
     - **coordinate**。节点既不是master，也不是data的话，就是调度者。调度者专门用来做请求的分发和聚合。当数据量很大的时候，就需要有专门的节点来做协调者，以提高索引和查询的速度

2. master选举的过程
    - 前置条件：
       - 1）只有是候选主节点（master：true）的节点才能成为主节点。
       - 2）最小主节点数（min_master_nodes）的目的是防止脑裂。
    - Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分；
　　获取主节点的核心入口为findMaster，选择主节点成功返回对应Master，否则返回null。
    - 选举流程大致描述如下：
       - 第一步：确认候选主节点数达标，elasticsearch.yml设置的值discovery.zen.minimum_master_nodes;
       - 第二步：对所有候选主节点根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。
       - 第三步：如果对某个节点的投票数达到一定的值（候选主节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。
    - 补充：这里的id为string类型。master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。

3. 如何解决ES集群的脑裂问题
    - 所谓集群脑裂，是指Elasticsearch集群中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master的情况。
    - 当集群master候选数量不小于3个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题；
    - 当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题



## 底层原理
1. 关键名词
   - 写入
      - **index buffer**。写入的请求会先放在这个buffer中，默认每隔1s refresh到filesystem cache中，形成排序段。
      - **tanslog**。事务日志，用来机器宕机时的请求恢复。
      - **filesystem cache**。内存中的排序段，默认每隔30s通过flush写入到lucene倒排索引段中。
      - **lucene 倒排索引段**。底层的倒排索引段，以一段一段的形式存储。
   - 读取
      - **filter cache**。过滤缓存，缓存filter请求的结果，加快查询。
      - **字典树（FST）**。加快倒排索引中key的查找。
      - **lucene 倒排索引段**。
      - **doc value**。正排索引,保存文档->字段的信息，以列式存储。用于排序和分组查询。


2. 索引数据的流程
这里的索引文档应该理解为文档写入ES，创建索引的过程。

    - **路由**。默认根据文档ID计算路由分片。hash(id) % 分片数
    - **副本是否存活**（quoram/one/all）。计算存活的副本数量是否满足要求。如果不满足则判断写入失败
    - **写入translog**。请求先写入事务日志，万一机器宕机后恢复请求。
    - **写入index buffer**。请求写入索引缓存。
    - **refresh**。默认每隔1s会将index buffer缓存内的请求形成倒排索引段，并写入filesystem cache
    - **flush**。每隔30s或者filesystem cache中缓存达到一定程度时，将filesystem cache中的段写入lucene磁盘中
    - **合并段**。lucene会定时将小的倒排索引段合并成大的索引段，以减小磁盘空间，并提升检索速度

   下面是详细的写入过程说明：
    - 第一步：客户端向集群某节点写入数据，发送请求。（如果没有指定路由/协调节点，请求的节点扮演协调节点的角色。）
    - 第二步：协调节点接受到请求后，默认使用文档ID参与计算（也支持通过routing），得到该文档属于哪个分片。随后请求会被转到另外的节点。
*路由算法：根据文档id或路由计算目标的分片id
shard = hash(document_id) % (num_of_primary_shards)*
    - 第三步：当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh；
    - 第四步：当然在某些情况下，存在Memery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush；
    - 第五步：在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。
    - 第六步：flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时。
    ![](https://raw.githubusercontent.com/roperluo32/images/master/image20191212074504.png)
    - 补充：关于Lucene的Segement。Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。（段合并）


3. 检索数据的流程
    - **协调节点向索引的各个分片发送子请求**。
    - **子请求的处理**
       - **filter过滤**。首先利用filter cache过滤，没有命中的话再去filesystem cache和lucene的各个倒排索引段中查找。并将过滤的结果缓存到filter cache
       - **query查询**。先查找filesystem cache中的数据，然后利用字典树去lucene的倒排索引数据段中去查找
       - **排序与分组**。用DocValue和FieldData Cache去做排序和分组
    - **合并子请求的返回结果**
    - **fetch**。根据合并后的结果，再去补充文档的详细内容


## 大数据存储优化
1. 每天2T的数据量，考虑怎么存储？
    - **部署层面**
      - 磁盘用SSD固态硬盘，有条件做Raid
      - 内存32G以上，最好64G
      - 关闭内存Swap。swap是性能杀手
      - 系统文件描述符调到尽量大。比如65535.因为lucene的倒排索引段需要占用很多文件描述符
      - Java Heap内存占用到总内存的一半，并且最多32G。要给系统内存留足够的空间来做filesystem cache。
    - **索引层面**
      - 基于mapping 模板+时间去做索引滚动。避免单个索引过大带来的问题
      - 大批量请求使用bulk，一般请求控制在5~15MB是不错的选择
      - 大数据量的请求使用scroll api分批请求
      
      - 对于大批量，实时性要求低的数据导入，可以减少refresh的间隔，放开合并段的限流，以及暂时关闭副本复制等方法，来提高大批量导入的效率
    - **存储层面**
      - 冷热数据分离。对冷数据定期做force_merge与shrink压缩或者清除


2. 对于GC方面，在使用ES时要注意什么？
    - 1）倒排词典的索引需要常驻内存，无法GC，需要监控data node上segment memory增长趋势。
    - 2）各类缓存，field cache, filter cache, indexing cache, bulk queue等等，要设置合理的大小，并且要应该根据最坏的情况来看heap是否够用，也就是各类缓存全部占满的时候，还有heap空间可以分配给其他任务吗？避免采用clear cache等“自欺欺人”的方式来释放内存。
    - 3）避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan & scroll api来实现。
    - 4）cluster stats驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过tribe node连接。
    - 5）想知道heap够不够，必须结合实际应用场景，并对集群的heap使用情况做持续的监控。


3. elasticsearch 索引数据多了怎么办，如何调优，部署
    - 索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。
    如何调优，正如问题1所说，这里细化一下：
    - **动态索引层面**
基于模板+时间+rollover api滚动创建索引，举例：设计阶段定义：blog索引的模板格式为：blog_index_时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线2的32次幂-1，索引存储达到了TB+甚至更大。一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。
     - **存储层面**
冷热数据分离存储，热数据（比如最近3天或者一周的数据），其余为冷数据。
对于冷数据不会再写入新数据，可以考虑定期force_merge加shrink压缩操作，节省存储空间和检索效率。
     - **部署层面**
一旦之前没有规划，这里就属于应急策略。
结合ES自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。


4. Elasticsearch在部署时，对Linux的设置有哪些优化方法
     - 1）关闭缓存swap; 
     - 2）堆内存设置为：Min（节点内存/2, 32GB）;
     - 3)设置最大文件句柄数；
     - 4）线程池+队列大小根据业务需要做调整； 
     - 5）磁盘存储raid方式——存储有条件使用RAID10，增加单节点性能以及避免单节点存储故障。

## 并发访问
1. 在并发情况下，ES如果保证读写一致？
   - 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
   - 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
   - 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。

## 实际使用
1. 说说你们公司ES的集群架构，索引数据大小，分片有多少，以及一些调优手段？
    - 我司有多个ES集群，下面列举其中一个。该集群有20个节点，根据数据类型和日期分库，每个索引根据数据量分片，比如日均1亿+数据的，控制单索引大小在200GB以内。　下面重点列举一些调优策略，仅是我做过的，不一定全面，如有其它建议或者补充欢迎留言。
    - 部署层面：
        - 1）最好是64GB内存的物理机器，但实际上32GB和16GB机器用的比较多，但绝对不能少于8G，除非数据量特别少，这点需要和客户方面沟通并合理说服对方。
        - 2）多个内核提供的额外并发远胜过稍微快一点点的时钟频率。
        - 3）尽量使用SSD，因为查询和索引性能将会得到显著提升。
        - 4）避免集群跨越大的地理距离，一般一个集群的所有节点位于一个数据中心中。
        - 5）设置堆内存：节点内存/2，不要超过32GB。一般来说设置export ES_HEAP_SIZE=32g环境变量，比直接写-Xmx32g -Xms32g更好一点。
        - 6）关闭缓存swap。内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个100微秒的操作可能变成10毫秒。 再想想那么多10微秒的操作时延累加起来。不难看出swapping对于性能是多么可怕。
        - 7）增加文件描述符，设置一个很大的值，如65535。Lucene使用了大量的文件，同时，Elasticsearch在节点和HTTP客户端之间进行通信也使用了大量的套接字。所有这一切都需要足够的文件描述符。
        - 8）不要随意修改垃圾回收器（CMS）和各个线程池的大小。
        - 9）通过设置gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。
    - 索引层面：
       - 1）使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。
       - 2）段合并：Elasticsearch默认值是20MB/s，对机械磁盘应该是个不错的设置。如果你用的是SSD，可以考虑提高到100-200MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的512MB到更大一些的值，比如1GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。
       - 3）如果你的搜索结果不需要近实时的准确度，考虑把每个索引的index.refresh_interval 改到30s。
       - 4）如果你在做大批量导入，考虑通过设置index.number_of_replicas: 0 关闭副本。
       - 5）需要大量拉取数据的场景，可以采用scan & scroll api来实现，而不是from/size一个大范围。
    - 存储层面：
       - 1）基于数据+时间滚动创建索引，每天递增数据。控制单个索引的量，一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。
       - 2）冷热数据分离存储，热数据（比如最近3天或者一周的数据），其余为冷数据。对于冷数据不会再写入新数据，可以考虑定期forc   e_merge加shrink压缩操作，节省存储空间和检索效率

## 其它知识
1. 文档ID生成的算法？
    - 自动生成的文档ID是由修改过的FakeID算法生成
    - 生成的ID是URL安全，Base64编码的20位字符的GUID

## 参考文档
- [Elasticsearch面试题](https://www.wenyuanblog.com/blogs/elasticsearch-interview-questions.html)
- [时间序列数据库的秘密 (2)——索引](https://www.infoq.cn/article/database-timestamp-02/?utm_source=infoq&utm_medium=related_content_link&utm_campaign=relatedContent_articles_clk)
- [Elasticsearch－基础介绍及索引原理分析](https://www.cnblogs.com/aspirant/p/11323890.html)
- [干货 | BAT等一线大厂 Elasticsearch面试题解读](https://juejin.im/entry/5c46d7c2e51d4551df6f2338)
- [分布式搜索引擎Elasticsearch面试题及一些概念](https://blog.itning.top/posts/elasticsearch/20190325-distributed-search-engine-elasticsearch-interview-questions-and-some-concepts)